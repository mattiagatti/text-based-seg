{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment images by using text prompts\n",
    "The purpose of this notebook is to illustrate different methods for segmenting images using text prompts. Text prompts are used to semantically identify the parts of the image to be segmented.\n",
    "\n",
    "## Segment Anything Model\n",
    "The Segment Anything Model (SAM) is the main component of the proposed methods for segmenting satellite imagery using text prompts. SAM produces high quality object masks from input prompts such as points or boxes, and can be used to generate masks for all objects in an image. However, it doesn't accept text prompts, and implementing this feature requires combining SAM with other visual foundation models.\n",
    "\n",
    "### SAM Automatic mask generator\n",
    "SAM can generate masks for all objects in an image. However, objects with the same semantic don't share the same mask index (different masks for each instance of an object). A notebook showing how to use this feature is available [here](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb).\n",
    "\n",
    "### SAM Predictor\n",
    "SAM is able to predict object masks given prompts that indicate the desired object. Ideally, if a box is given as a prompt, SAM will segment the main object within the box, while if a point is given, SAM will segment the object to which the point belongs. A notebook showing how to use this feature is available [here](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m venv .venv\n",
    "# . .venv/bin/activate\n",
    "# pip install ipykernel\n",
    "# python -m ipykernel install --user --name text-based-seg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the torch version that matches your installed CUDA version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu111/torch_stable.html\n",
    "%pip install ipywidgets pandas scikit-learn jinja2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change kernel with the new one before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Grounding DINO\n",
    "!git clone 'https://github.com/IDEA-Research/GroundingDINO.git'\n",
    "%cd GroundingDINO/\n",
    "%pip install -e .\n",
    "%cd ..\n",
    "%mkdir weights\n",
    "%cd weights\n",
    "!wget -N 'https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth'\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Segment Anything Model\n",
    "%pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "%cd weights\n",
    "!wget -N 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth'\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install CLIP repository\n",
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download CLIP Surgery repository\n",
    "!git clone https://github.com/xmed-lab/CLIP_Surgery.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want many images to play with you can download the Cityscapes dataset. The image used for different examples in this notebook is from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register https://www.cityscapes-dataset.com/register/\n",
    "# change myusername and mypassword with your login data\n",
    "# download Cityscapes dataset\n",
    "%mkdir -p data/cityscapes\n",
    "%cd data/cityscapes\n",
    "!wget --keep-session-cookies --save-cookies=cookies.txt --post-data 'username=myusername&password=mypassword&submit=Login' 'https://www.cityscapes-dataset.com/login/'\n",
    "!wget -nc --load-cookies cookies.txt --content-disposition 'https://www.cityscapes-dataset.com/file-handling/?packageID=1'\n",
    "!wget -nc --load-cookies cookies.txt --content-disposition 'https://www.cityscapes-dataset.com/file-handling/?packageID=3'\n",
    "!unzip -n gtFine_trainvaltest.zip\n",
    "!unzip -n leftImg8bit_trainvaltest.zip\n",
    "%cd ../.."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart runtime to load new packages before going forward otherwise groundingdino will not be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define methods to show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a mask and an image, this method highlights the parts of the image referenced by the mask.\n",
    "def draw_mask(mask, image):\n",
    "    color_map = np.array([255/255, 51/255, 51/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.reshape(h, w, 1) * color_map.reshape(1, 1, -1)\n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    mask_image_pil = Image.fromarray((mask * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
    "\n",
    "# plot image, ground truth and prediction\n",
    "def plot_result(image, gt, pred):\n",
    "    _, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    # show input\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Image\")\n",
    "    # show prediction\n",
    "    axes[1].imshow(gt)\n",
    "    axes[1].set_title(\"Ground truth\")\n",
    "    # show target\n",
    "    axes[2].imshow(pred)\n",
    "    axes[2].set_title(\"Prediction\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.imshow()\n",
    "\n",
    "# show annotations of object segmentation masks\n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "\n",
    "    return (img * 255).astype(np.uint8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grounding DINO + SAM\n",
    "Grounding DINO is a model used to detect objects given one or more text inputs. Grounding DINO return a box for each detection and then all these boxes can become an input of SAM to perform the segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "from segment_anything import SamPredictor, sam_model_registry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundingDINO_checkpoint = Path(\"weights\", \"groundingdino_swint_ogc.pth\")\n",
    "groundingDINO = load_model(Path(\"GroundingDINO\", \"groundingdino\", \"config\", \"GroundingDINO_SwinT_OGC.py\"), groundingDINO_checkpoint)\n",
    "groundingDINO.to(device)\n",
    "\n",
    "sam_checkpoint = Path(\"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "model_type = \"vit_h\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device)\n",
    "sam_predictor = SamPredictor(sam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define predict methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the boxes of the detections from an image given a text_prompt\n",
    "def predict_dino(image_path, text_prompt):\n",
    "    image_source, image = load_image(image_path)\n",
    "    if type(text_prompt) == str:\n",
    "        text_prompt = [text_prompt]\n",
    "\n",
    "    boxes_lst = []\n",
    "    for prompt in text_prompt:\n",
    "        boxes, logits, phrases = predict(\n",
    "            model=groundingDINO,\n",
    "            image=image,\n",
    "            caption=prompt,\n",
    "            box_threshold=0.35,\n",
    "            text_threshold=0.25,\n",
    "            device=device\n",
    "        )\n",
    "        annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "        annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)\n",
    "        boxes_lst.extend(boxes)\n",
    "\n",
    "        if len(boxes_lst) > 0:\n",
    "            boxes_lst = torch.stack(boxes_lst)\n",
    "            H, W, _ = image_source.shape\n",
    "            boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes_lst) * torch.Tensor([W, H, W, H])\n",
    "        else:\n",
    "            boxes_xyxy = None\n",
    "    return boxes_xyxy, annotated_frame\n",
    "\n",
    "# returns the segmentation mask given a set of boxes and/or points\n",
    "def predict_sam(image_path, boxes=None, point_coords=None, point_labels=None):\n",
    "    image_source, image = load_image(image_path)\n",
    "    sam_predictor.set_image(image_source)\n",
    "\n",
    "    if boxes != None:  # Grounding DINO\n",
    "        boxes = sam_predictor.transform.apply_boxes_torch(boxes.to(device), image_source.shape[:2])\n",
    "        masks, _, _ = sam_predictor.predict_torch(\n",
    "            point_coords = None,\n",
    "            point_labels = None,\n",
    "            boxes = boxes,\n",
    "            multimask_output = False\n",
    "        )\n",
    "\n",
    "        masks = ((masks.sum(dim=0)>0)[0]*1).cpu().numpy()\n",
    "    else:  # CLIPS\n",
    "        masks, _, _ = sam_predictor.predict(\n",
    "            point_coords = point_coords,\n",
    "            point_labels = point_labels,\n",
    "            multimask_output = False\n",
    "        )\n",
    "        masks = np.array(masks)[0, :, :]\n",
    "\n",
    "    annotated_frame_with_mask = draw_mask(masks, image_source)\n",
    "\n",
    "    return masks, annotated_frame_with_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = Path(\"data\", \"cityscapes\", \"leftImg8bit\", \"val\", \"lindau\", \"lindau_000000_000019_leftImg8bit.png\")\n",
    "text_prompt = \"tree\"\n",
    "\n",
    "# get boxes of objects related to the text prompt\n",
    "boxes_xyxy, annotated_frame = predict_dino(image_path, [text_prompt])\n",
    "# for each box segment the object inside\n",
    "masks, annotated_frame_with_mask = predict_sam(image_path, boxes=boxes_xyxy)\n",
    "\n",
    "print(\"Grounding DINO Output\")\n",
    "plt.imshow(annotated_frame)\n",
    "plt.show()\n",
    "\n",
    "print(\"SAM Output\")\n",
    "plt.imshow(annotated_frame_with_mask)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Surgery + SAM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP Surgery (CLIPS) is a model made to enhance explainability of CLIP. CLIP Surgery can be used to convert text to point prompts and then these points can be used by SAM to perform the segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CLIP_Surgery.clip as clips"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips_model, _ = clips.load(\"CS-ViT-B/16\", device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the points of the features from an image given a text_prompt\n",
    "def predict_CLIPS(image_path, text_prompt):\n",
    "    pil_img = Image.open(image_path)\n",
    "    image_source = np.array(pil_img)\n",
    "\n",
    "    if type(text_prompt) == str:\n",
    "        text_prompt = [text_prompt]\n",
    "    \n",
    "    # you can change height and width of the resize transformation based on the dataset\n",
    "    height, width = 1024, 1024\n",
    "\n",
    "    # predict\n",
    "    preprocess =  T.Compose([T.Resize((height, width), interpolation=T.InterpolationMode.BICUBIC),\n",
    "                             T.ToTensor(), T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))])\n",
    "    cv2_img = cv2.cvtColor(image_source, cv2.COLOR_RGB2BGR)\n",
    "    image = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        # CLIP architecture surgery acts on the image encoder\n",
    "        image_features = clips_model.encode_image(image)\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # Prompt ensemble for text features with normalization\n",
    "        text_features = clips.encode_text_with_prompt_ensemble(clips_model, text_prompt, device)\n",
    "\n",
    "        # Extract redundant features from an empty string\n",
    "        redundant_features = clips.encode_text_with_prompt_ensemble(clips_model, [\"\"], device)\n",
    "\n",
    "        # CLIP feature surgery with costum redundant features\n",
    "        similarity = clips.clip_feature_surgery(image_features, text_features, redundant_features)[0]\n",
    "            \n",
    "        # Inference SAM with points from CLIP Surgery\n",
    "        coords, labels = clips.similarity_map_to_points(similarity[1:, 0], cv2_img.shape[:2], t=0.8)\n",
    "\n",
    "        # Annotate with points\n",
    "        annotated_frame = cv2_img.copy()\n",
    "        for i, [x, y] in enumerate(coords):\n",
    "            cv2.circle(annotated_frame, (x, y), 3, (0, 102, 255) if labels[i] == 1 else (255, 102, 51), 3)\n",
    "        annotated_frame = cv2.cvtColor(annotated_frame.astype('uint8'), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        return np.array(coords), labels, annotated_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = Path(\"data\", \"cityscapes\", \"leftImg8bit\", \"val\", \"lindau\", \"lindau_000000_000019_leftImg8bit.png\")\n",
    "text_prompt = \"tree\"\n",
    "\n",
    "# get points of objects related to the text prompt\n",
    "coords, labels, annotated_frame = predict_CLIPS(image_path, [text_prompt])\n",
    "# segment the objects related with the given points\n",
    "masks, annotated_frame_with_mask = predict_sam(image_path, point_coords=coords, point_labels=labels)\n",
    "\n",
    "print(\"CLIP Surgery Output\")\n",
    "plt.imshow(annotated_frame)\n",
    "plt.show()\n",
    "\n",
    "print(\"SAM Output\")\n",
    "plt.imshow(annotated_frame_with_mask)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAM + CLIP\n",
    "CLIP is a model used to compute image-text similarity. Another method to segment based on a text prompt is to use SAM Automatic mask generation to generate masks for each object and then let CLIP compute the similarity between the text prompt and each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from segment_anything import SamAutomaticMaskGenerator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_mask_generator = SamAutomaticMaskGenerator(sam, points_per_side=10)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/16\", device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sam_auto(image_path):\n",
    "    image_source, _ = load_image(image_path)\n",
    "    \n",
    "    # use SAM to generate masks\n",
    "    segmented_frame_masks = generic_mask_generator.generate(image_source)\n",
    "\n",
    "    # object annotations\n",
    "    annotated_frame = show_anns(segmented_frame_masks)\n",
    "    annotated_frame = annotated_frame[:, :, :3]\n",
    "\n",
    "    return segmented_frame_masks, annotated_frame\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def retriev(elements, search_text):\n",
    "    preprocessed_images = [clip_preprocess(image).to(device) for image in elements]\n",
    "    tokenized_text = clip.tokenize([search_text]).to(device)\n",
    "    stacked_images = torch.stack(preprocessed_images)\n",
    "    image_features = clip_model.encode_image(stacked_images)\n",
    "    text_features = clip_model.encode_text(tokenized_text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    probs = 100. * image_features @ text_features.T\n",
    "    return probs[:, 0].softmax(dim=0)\n",
    "\n",
    "\n",
    "def get_indices_of_values_above_threshold(values, threshold):\n",
    "    return [i for i, v in enumerate(values) if v > threshold]\n",
    "\n",
    "\n",
    "def segment_image(image, segmentation_mask):\n",
    "    image_array = image\n",
    "    segmented_image_array = np.zeros_like(image_array)\n",
    "    segmented_image_array[segmentation_mask] = image_array[segmentation_mask]\n",
    "    segmented_image = Image.fromarray(segmented_image_array)\n",
    "    black_image = Image.new(\"RGB\", image.shape[:2], (0, 0, 0))\n",
    "    transparency_mask = np.zeros_like(segmentation_mask, dtype=np.uint8)\n",
    "    transparency_mask[segmentation_mask] = 255\n",
    "    transparency_mask_image = Image.fromarray(transparency_mask, mode='L')\n",
    "    black_image.paste(segmented_image, mask=transparency_mask_image)\n",
    "    return black_image\n",
    "\n",
    "\n",
    "def convert_box_xywh_to_xyxy(box):\n",
    "    x1 = box[0]\n",
    "    y1 = box[1]\n",
    "    x2 = box[0] + box[2]\n",
    "    y2 = box[1] + box[3]\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "\n",
    "def predict_CLIP(image_path, segmented_frame_masks, text_prompt):\n",
    "    image_source, _ = load_image(image_path)\n",
    "\n",
    "    if type(text_prompt) == str:\n",
    "        text_prompt = [text_prompt]\n",
    "\n",
    "    # Cut out all masks\n",
    "    cropped_boxes = []\n",
    "    for mask in segmented_frame_masks:\n",
    "        cropped_boxes.append(segment_image(image_source, mask[\"segmentation\"]).crop(convert_box_xywh_to_xyxy(mask[\"bbox\"])))  \n",
    "        \n",
    "    indices_lst = []\n",
    "    for prompt in text_prompt:\n",
    "        scores = retriev(cropped_boxes, prompt)\n",
    "        indices = get_indices_of_values_above_threshold(scores, 0.05)\n",
    "        indices_lst.extend(indices)      \n",
    "            \n",
    "    segmentation_masks = []\n",
    "    for seg_idx in np.unique(indices_lst):\n",
    "        segmentation_mask_image = segmented_frame_masks[seg_idx][\"segmentation\"]\n",
    "        segmentation_masks.append(segmentation_mask_image)\n",
    "    segmentation_masks = np.array(segmentation_masks).sum(axis=0)>0\n",
    "\n",
    "    if segmentation_masks.ndim == 0:\n",
    "        segmentation_masks = np.zeros(shape=image_source.shape[0:2])\n",
    "\n",
    "    annotated_frame_with_mask = draw_mask(segmentation_masks, image_source)\n",
    "\n",
    "    return segmentation_masks, annotated_frame_with_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = Path(\"data\", \"cityscapes\", \"leftImg8bit\", \"val\", \"lindau\", \"lindau_000000_000019_leftImg8bit.png\")\n",
    "text_prompt = \"tree\"\n",
    "\n",
    "# get object segmentation masks\n",
    "segmented_frame_masks, annotated_frame = predict_sam_auto(image_path)\n",
    "# check which objects are related to the text prompt\n",
    "masks, annotated_frame_with_mask = predict_CLIP(image_path, segmented_frame_masks, text_prompt)\n",
    "\n",
    "print(\"SAM Output\")\n",
    "plt.imshow(annotated_frame)\n",
    "plt.show()\n",
    "\n",
    "print(\"CLIP Output\")\n",
    "plt.imshow(annotated_frame_with_mask)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the models with different text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = Path(\"data\", \"cityscapes\", \"leftImg8bit\", \"val\", \"lindau\", \"lindau_000000_000019_leftImg8bit.png\")\n",
    "text_prompts = [\"banner\", \"car\", \"plate\", \"road\", \"roof\", \"tree\"]\n",
    "\n",
    "for text_prompt in text_prompts:\n",
    "    # DINO + SAM\n",
    "    boxes_xyxy, _ = predict_dino(image_path, [text_prompt])\n",
    "    _, annotated_frame_with_mask_dino = predict_sam(image_path, boxes=boxes_xyxy)\n",
    "    # CLIPS + SAM\n",
    "    coords, labels, _ = predict_CLIPS(image_path, [text_prompt])\n",
    "    _, annotated_frame_with_mask_clips = predict_sam(image_path, point_coords=coords, point_labels=labels)\n",
    "    # SAM + CLIP\n",
    "    segmented_frame_masks, _ = predict_sam_auto(image_path)\n",
    "    _, annotated_frame_with_mask_clip = predict_CLIP(image_path, segmented_frame_masks, text_prompt)\n",
    "    # plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    fig.suptitle(text_prompt.capitalize())\n",
    "    axes[0].set_title(\"Grounding DINO + SAM\")\n",
    "    axes[0].imshow(annotated_frame_with_mask_dino)\n",
    "    axes[1].set_title(\"CLIPS + SAM\")\n",
    "    axes[1].imshow(annotated_frame_with_mask_clips)\n",
    "    axes[2].set_title(\"SAM + CLIP\")\n",
    "    axes[2].imshow(annotated_frame_with_mask_clip)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying the three models with different prompts, visual inspection didn't  show better results from a model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "Compute metrics over the Cityscapes dataset. Metrics are computed for each text prompt (= the name of the class) over the ground truth. The same pixel can be of different classes because the resulting masks intersection is not always empty. For this reason a confusion matrix can't be built properly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Cityscapes dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cityscapes(Dataset):\n",
    "    def __init__(self, root, split='train', mode='fine', target_type='semantic', transform=None):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.mode = 'gtFine'\n",
    "        self.target_type = target_type\n",
    "        self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
    "\n",
    "        self.targets_dir = os.path.join(self.root, self.mode, split)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.split = split\n",
    "        self.images = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for city in os.listdir(self.images_dir):\n",
    "            img_dir = os.path.join(self.images_dir, city)\n",
    "            target_dir = os.path.join(self.targets_dir, city)\n",
    "\n",
    "            for file_name in os.listdir(img_dir):\n",
    "                self.images.append(os.path.join(img_dir, file_name))\n",
    "                target_name = '{}_{}'.format(file_name.split('_leftImg8bit')[0],\n",
    "                                             self._get_target_suffix(self.mode, self.target_type))\n",
    "                self.targets.append(os.path.join(target_dir, target_name))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        target = Image.open(self.targets[index])\n",
    "        target = np.array(target)\n",
    "        return image_path, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def _get_target_suffix(self, mode, target_type):\n",
    "        if target_type == 'instance':\n",
    "            return '{}_instanceIds.png'.format(mode)\n",
    "        elif target_type == 'semantic':\n",
    "            return '{}_labelIds.png'.format(mode)\n",
    "        elif target_type == 'color':\n",
    "            return '{}_color.png'.format(mode)\n",
    "        elif target_type == 'polygon':\n",
    "            return '{}_polygons.json'.format(mode)\n",
    "        elif target_type == 'depth':\n",
    "            return '{}_disparity.png'.format(mode)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subset of the test_dataset is used, otherwise metrics evaluation takes too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Cityscapes('./data/cityscapes', split='val', mode='fine', target_type='semantic')\n",
    "random.seed(42)\n",
    "test_subset_indices = random.choices(range(0, len(test_dataset)), k=int(len(test_dataset) * 0.1))\n",
    "test_subset = Subset(test_dataset, test_subset_indices)\n",
    "n_cpu = os.cpu_count()\n",
    "test_dataloader = DataLoader(test_subset, batch_size=1, shuffle=False, num_workers=n_cpu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define index-prompt relation\n",
    "The first value is the index, the second element is the name of the class (and also the input prompt).<br>\n",
    "e.g. each pixel that belongs to road class has 7 as value in Cityscapes GT. Ideally, if one of the models is prompted with the string \"road\" I expect that the output mask is the same as Cityscapes GT considering only the 7 index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\n",
    "    (7, 'road'),\n",
    "    (8, 'sidewalk'),\n",
    "    (11, 'building'),\n",
    "    (12, 'wall'),\n",
    "    (13, 'fence'),\n",
    "    (17, 'pole'),\n",
    "    (18, 'pole group'),\n",
    "    (19, 'traffic light'),\n",
    "    (20, 'traffic sign'),\n",
    "    (21, 'vegetation'),\n",
    "    (22, 'terrain'),\n",
    "    (23, 'sky'),\n",
    "    (24, 'person'),\n",
    "    (25, 'rider'),\n",
    "    (26, 'car'),\n",
    "    (27, 'truck'),\n",
    "    (28, 'bus'),\n",
    "    (31, 'train'),\n",
    "    (32, 'motorcycle'),\n",
    "    (33, 'bicycle'),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred):\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "metrics = np.zeros(shape=(3, len(classes), 3))\n",
    "for i, (image_path, gt) in enumerate(test_dataloader):\n",
    "    image_path, gt = image_path[0], gt.squeeze()\n",
    "\n",
    "    for j, (index, text_prompt) in enumerate(classes):\n",
    "        print(f\"{i * len(classes) + j + 1}/{len(test_dataloader) * len(classes)}\")\n",
    "        gt_index = (gt == index).numpy().astype(int)\n",
    "        \n",
    "        # DINO + SAM\n",
    "        boxes_xyxy, _ = predict_dino(image_path, [text_prompt])\n",
    "        mask_dino, _ = predict_sam(image_path, boxes=boxes_xyxy)\n",
    "        # CLIPS + SAM\n",
    "        coords, labels, _ = predict_CLIPS(image_path, [text_prompt])\n",
    "        mask_clips, _ = predict_sam(image_path, point_coords=coords, point_labels=labels)\n",
    "        # SAM + CLIP\n",
    "        segmented_frame_masks, _ = predict_sam_auto(image_path)\n",
    "        mask_clip, _ = predict_CLIP(image_path, segmented_frame_masks, text_prompt)\n",
    "        # compute metrics\n",
    "        metrics[0, j] = metrics[0, j] + get_metrics(gt_index, mask_dino)\n",
    "        metrics[1, j] = metrics[1, j] + get_metrics(gt_index, mask_clips)\n",
    "        metrics[2, j] = metrics[2, j] + get_metrics(gt_index, mask_clip)\n",
    "\n",
    "metrics = metrics / len(test_dataloader)\n",
    "np.save('metrics.npy', metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print comparison results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export results as LaTeX tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "metrics = np.load('metrics.npy')\n",
    "dino_metrics, clips_metrics, clip_metrics = metrics[0], metrics[1], metrics[2]\n",
    "\n",
    "dino_df = pd.DataFrame({\n",
    "    'class': [x[1] for x in classes],\n",
    "    'accuracy': dino_metrics[:, 0],\n",
    "    'precision': dino_metrics[:, 1],\n",
    "    'recall': dino_metrics[:, 2]\n",
    "})\n",
    "dino_df.loc[len(dino_df)] = {\n",
    "    'class': 'Overall',\n",
    "    'accuracy': dino_df['accuracy'].mean(),\n",
    "    'precision': dino_df['precision'].mean(),\n",
    "    'recall': dino_df['recall'].mean()\n",
    "}\n",
    "dino_df = dino_df.round(6)\n",
    "print(dino_df.to_latex())\n",
    "\n",
    "clips_df = pd.DataFrame({\n",
    "    'class': [x[1] for x in classes],\n",
    "    'accuracy': clips_metrics[:, 0],\n",
    "    'precision': clips_metrics[:, 1],\n",
    "    'recall': clips_metrics[:, 2]\n",
    "})\n",
    "clips_df.loc[len(clips_df)] = {\n",
    "    'class': \"Overall\",\n",
    "    'accuracy': clips_df['accuracy'].mean(),\n",
    "    'precision': clips_df['precision'].mean(),\n",
    "    'recall': clips_df['recall'].mean()\n",
    "}\n",
    "clips_df = clips_df.round(6)\n",
    "print(clips_df.to_latex())\n",
    "\n",
    "clip_df = pd.DataFrame({\n",
    "    'class': [x[1] for x in classes],\n",
    "    'accuracy': clip_metrics[:, 0],\n",
    "    'precision': clip_metrics[:, 1],\n",
    "    'recall': clip_metrics[:, 2]\n",
    "})\n",
    "clip_df.loc[len(clip_df)] = {\n",
    "    'class': \"Overall\",\n",
    "    'accuracy': clip_df['accuracy'].mean(),\n",
    "    'precision': clip_df['precision'].mean(),\n",
    "    'recall': clip_df['recall'].mean()\n",
    "}\n",
    "clip_df = clip_df.round(6)\n",
    "print(clip_df.to_latex())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding Remarks\n",
    "Segmentation based on text prompts is not reliable.\n",
    "\n",
    "There are other combinations worth trying:\n",
    "- Grounding DINO, then SAM and finally CLIP\n",
    "- CLIP Surgery, then SAM and finally CLIP\n",
    "- Both Grounding DINO and CLIP Surgery, then SAM and finally CLIP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-based-seg",
   "language": "python",
   "name": "text-based-seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
